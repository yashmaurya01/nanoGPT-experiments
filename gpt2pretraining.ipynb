{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashmaurya01/nanoGPT-experiments/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY6VxIAB0R62",
        "outputId": "e7a94ed4-3fdb-4e2a-ec80-07867221673d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-28 05:55:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-28 05:55:32 (28.4 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Zped78gd0R63"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SeeYhnL0R63",
        "outputId": "4ea4a993-d4ce-4d24-8669-d8051b805d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_2YQKlO0R63",
        "outputId": "c09438ae-74eb-4574-8ea2-f93d4a420f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1115394\n"
          ]
        }
      ],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IJqay_d0R63",
        "outputId": "3e39dcf2-1e35-42ff-b2ba-5084155e0f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpOPDiCu0R63",
        "outputId": "6adf3f94-ebb2-42fe-f664-03a158ba5c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 63, 39, 57, 46]\n",
            "hii my name is yash\n"
          ]
        }
      ],
      "source": [
        "##character level language model\n",
        "import torch\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    if isinstance(l, torch.Tensor):\n",
        "        l = l.tolist()\n",
        "    if isinstance(l, int):\n",
        "        return itos[l]\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode('hii my name is yash'))\n",
        "print(decode(encode('hii my name is yash')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GEzfY_Ud0R63"
      },
      "outputs": [],
      "source": [
        "# import tiktoken\n",
        "\n",
        "# enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# print(enc.encode('yash'))\n",
        "# print(enc.decode(enc.encode('yash')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfV1kx_W0R63",
        "outputId": "ae864ae7-0763-4bab-cf4e-351ce8f629a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "#encode the shakespeare text\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long) #torch.long is equivalent to torch.int64\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RxPwQ6B0R63",
        "outputId": "45bb4605-a7ce-4ab4-b31b-e2f46fdb55d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1003854]) torch.Size([111540])\n"
          ]
        }
      ],
      "source": [
        "#train-val split 90-10\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(train_data.shape, val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "htdOAn1g0R63",
        "outputId": "90f9b85d-592d-49c3-8729-ef1abca77606"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'t'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "decode(train_data[block_size:block_size+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmnAtZV90R63",
        "outputId": "9d853f7c-53af-457f-b98d-117f31d9aeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is F the target is i\n",
            "when input is Fi the target is r\n",
            "when input is Fir the target is s\n",
            "when input is Firs the target is t\n",
            "when input is First the target is  \n",
            "when input is First  the target is C\n",
            "when input is First C the target is i\n",
            "when input is First Ci the target is t\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size+1]\n",
        "y = train_data[1:block_size+2]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {decode(context)} the target is {decode(target)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbczFfQU0R64",
        "outputId": "7ede6472-2e49-43ea-9bf9-bfa984ceb9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "import torch\n",
        "device = torch.device(\"cpu\")  # Start with CPU as default\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    device = torch.device(\"mps\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbUylTO30R64",
        "outputId": "167377d9-2165-449c-f814-edf369f596c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embed = 384\n",
        "num_heads = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "max_iters = 2000\n",
        "eval_iters = 200\n",
        "lr = 3e-4\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "K6oT0ZNA0R64"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "dkRULAdq0R64"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by non-linearity\"\"\"\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4* n_embed), # 4 multiplied based on the transformers paper scaled down version\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        #compute attention scores\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        #aggregation of values\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed) #allows the model to focus on specific aspects of the input data by projecting it into a space where relationships between different parts of the sequence can be easily assessed, making the attention mechanism more efficient and effective\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenating over the channel dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computations\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) #x + is the residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        # self.selfattention_heads = MultiHeadAttention(num_heads, n_embed//num_heads) #i.e. 4 heads of 8-dimensional self-attention\n",
        "        # self.feedforward = FeedForward(n_embed)\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     nn.LayerNorm(n_embed)\n",
        "        # )\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = num_heads) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) #final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "        # better init, covered in the follow-up original GPT video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        #idx and targets are both (B, T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #B, T, n_embed(C)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = device)) #T, n_embed\n",
        "        x = token_embeddings + position_embeddings #B, T, n_embed\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        # x = self.selfattention_heads(x) #apply one head of self-attention\n",
        "        # x = self.feedforward(x)\n",
        "        logits = self.lm_head(x) #B, T, vocab_size\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        idx = idx.to(device)\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            #get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            #focus only on last time step\n",
        "            logits = logits[:, -1, :]\n",
        "            #apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            #sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# logits, loss = model(x, y)\n",
        "\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "\n",
        "# idx = torch.zeros((1, 1), dtype=torch.long) #starting with a newline token\n",
        "# print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKiXtQK90R64",
        "outputId": "cd2127d2-f197-480a-f063-1f2683375c67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/2000 [01:02<14:21:14, 25.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 3.6488, val loss 3.6718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 202/2000 [03:40<7:02:42, 14.11s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 200: train loss 2.4000, val loss 2.4304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 402/2000 [06:17<6:14:24, 14.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 400: train loss 1.9308, val loss 2.0287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 602/2000 [08:53<5:26:49, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 600: train loss 1.6222, val loss 1.7917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 802/2000 [11:30<4:39:59, 14.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 800: train loss 1.4780, val loss 1.6734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1002/2000 [14:06<3:53:19, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1000: train loss 1.3923, val loss 1.6105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 1202/2000 [16:42<3:06:34, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1200: train loss 1.3332, val loss 1.5761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 1402/2000 [19:18<2:19:45, 14.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1400: train loss 1.2875, val loss 1.5367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 1602/2000 [21:55<1:33:01, 14.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1600: train loss 1.2503, val loss 1.5153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 1802/2000 [24:31<46:15, 14.02s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1800: train loss 1.2240, val loss 1.5104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [27:06<00:00,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1999: train loss 1.1882, val loss 1.4933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for steps in tqdm(range(max_iters)):\n",
        "    model.train()\n",
        "    xb, yb = get_batch('train')\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    model.to(device)\n",
        "    # print(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % eval_iters == 0 or steps == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsG_qL8b0R64",
        "outputId": "e8a2dcf0-a4fd-41a7-a96c-97cca82f9cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "OXFORDANLEY:\n",
            "Ay, shall I he3 KING HENRY VI:\n",
            "How now, Northumble should she dear.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Senless they a conquer provered with a war,\n",
            "To with ba prayers fr a joy abhoad himself\n",
            "And posterity and Glod, instering kingdomy\n",
            "That did ewith feast word you contented:\n",
            "All mess a turn this leans almost.\n",
            "The hour true is in great sworn: you have littled a word,\n",
            "Win play still as i' the fair abll,\n",
            "And too harm floces with a disperited of beje\n",
            "I had womaning all day encounted:\n",
            "Therefore the shall wealling the king his holy case,\n",
            "Stand from my mornsal thanks malices,\n",
            "Though the seople doth good, dost general.\n",
            "\n",
            "First Servant:\n",
            "O hath remedy, and loathed was the lack,\n",
            "Or, these weed from the treasons satoch\n",
            "Are ne'er feel. Although, seeing hates been prival,\n",
            "One a dancoured nobour?\n",
            "\n",
            "MENENIUS:\n",
            "My brave is answer my heads!\n",
            "And would yet here's a prated I say; I'll keep\n",
            "The edge pity for Cominius, and rememberal.\n",
            "\n",
            "MENIUS:\n",
            "When it\n",
            "Was datches; for thick? that's treamily, another neither?\n",
            "\n",
            "MEssenger\n"
          ]
        }
      ],
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long).to(device) #starting with a newline token\n",
        "print(decode(model.generate(idx, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PY5RWeRx0R64"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRTo8M1l0R64",
        "outputId": "219b05a0-41dc-4488-942e-89e0b2cfaf48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.174387269895637"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "-np.log(1/65)\n",
        "#this is the expected loss for a random guess,\n",
        "# but since we are getting 4.27 it means our model has some extra entropy and the loss is higher than expected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItTqT9sR0R64"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs9he-nk0R64",
        "outputId": "5a168ff0-3e37-41a4-db0b-2fdfb1629c66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " #toy example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single head self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) #(B,T,16)\n",
        "q = query(x) #(B, T, 16)\n",
        "v = value(x) #(B, T, 16)\n",
        "\n",
        "weights = q @ k.transpose(-2, -1) #(B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# weights = torch.zeros(T,T)\n",
        "weights = weights.masked_fill(tril==0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "out = weights @ v\n",
        "\n",
        "out.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFBR43Du0R64",
        "outputId": "f72d7d74-50c7-48bd-c9bd-6e9348408bfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AKVgwpBF0R64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

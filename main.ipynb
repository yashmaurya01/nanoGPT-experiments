{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-27 15:10:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-12-27 15:10:43 (14.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 63, 39, 57, 46]\n",
      "hii my name is yash\n"
     ]
    }
   ],
   "source": [
    "##character level language model\n",
    "\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    if isinstance(l, torch.Tensor):\n",
    "        l = l.tolist()\n",
    "    if isinstance(l, int):\n",
    "        return itos[l]\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hii my name is yash'))\n",
    "print(decode(encode('hii my name is yash')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# print(enc.encode('yash'))\n",
    "# print(enc.decode(enc.encode('yash')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "#encode the shakespeare text\n",
    "\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) #torch.long is equivalent to torch.int64\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "#train-val split 90-10\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "decode(train_data[block_size:block_size+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is F the target is i\n",
      "when input is Fi the target is r\n",
      "when input is Fir the target is s\n",
      "when input is Firs the target is t\n",
      "when input is First the target is  \n",
      "when input is First  the target is C\n",
      "when input is First C the target is i\n",
      "when input is First Ci the target is t\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+2]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {decode(context)} the target is {decode(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8231, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "fW.xwni,&HVGYHy3MP;yMDsxZ.!z,v-x$NknAukgY&SYW$Ggs;IYaPc-cTqC !R3fF!\n",
      "\n",
      "Zg-$wcfCA?WuF:dP-ASfF?:koZHSsJA\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #B, T, C\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "eval_iters = 20\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) #starting with a newline token\n",
    "print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.5075936317443848\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UKENTUCA:\n",
      "A: bere-\n",
      "Herih cave,\n",
      "Me p th,\n",
      "PAThagusorthatheve my LIOxffice sks; thintonges man bims Ne.\n",
      "HAnt prd ffr itass inilfrthyorss ard themert d athambour'shomese ang o's, decowa ato\n",
      "Th,\n",
      "BIILisid.\n",
      "MPor icechatrevand;\n",
      "Herert isthe ca e mbus!\n",
      "COLAn pry owouilld\n",
      "Thay de wir:\n",
      "Whind w ilound ty, hethalod l, yer tcratem be argh-\n",
      "NELLIUCLINIsoce s thatllid aigo thod, atemen schak br, awor he ishee my uig witot h thesin than t sheramof akshee y.\n",
      "Yo, misucar MPOLesthn tibur, s;\n",
      "Stist bu IO, ny the murunt nard An y turyrinteruthownshe men te\n",
      "To chanenessar hisolouscourgofterd, r by.\n",
      "Andonin ipoumemed f f oumy mior s thitartomaveyot ncor'\n",
      "\n",
      "We the And alers IENGUngis bthods fe h t IOLakndoouco aly t y, stheabet Wo eisondan we.\n",
      "\n",
      "MNERUMA id ge bupelacosenmur! angrnde\n",
      "\n",
      "Yory thathe m ISThe 'd n, fau?\n",
      "t ave RKI sonsioron:\n",
      "MAn n\n",
      "\n",
      "Baisheswicu ar meshaloutsewoutyomatheledenomugs; shit s,\n",
      "ADUENCormalindie oimmbofoun:\n",
      "Ford, XE:\n",
      "he the, llimengash hirsowig agimy sertacous st fame t swarwiowhthe th me ie, \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long) #starting with a newline token\n",
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.174387269895637)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "-np.log(1/65)\n",
    "#this is the expected loss for a random guess, \n",
    "# but since we are getting 4.27 it means our model has some extra entropy and the loss is higher than expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

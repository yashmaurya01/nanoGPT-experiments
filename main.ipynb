{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashmaurya01/nanoGPT-experiments/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY6VxIAB0R62",
        "outputId": "e7a94ed4-3fdb-4e2a-ec80-07867221673d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-28 05:55:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-12-28 05:55:32 (28.4 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Zped78gd0R63"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SeeYhnL0R63",
        "outputId": "4ea4a993-d4ce-4d24-8669-d8051b805d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_2YQKlO0R63",
        "outputId": "c09438ae-74eb-4574-8ea2-f93d4a420f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IJqay_d0R63",
        "outputId": "3e39dcf2-1e35-42ff-b2ba-5084155e0f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpOPDiCu0R63",
        "outputId": "6adf3f94-ebb2-42fe-f664-03a158ba5c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 63, 39, 57, 46]\n",
            "hii my name is yash\n"
          ]
        }
      ],
      "source": [
        "##character level language model\n",
        "import torch\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    if isinstance(l, torch.Tensor):\n",
        "        l = l.tolist()\n",
        "    if isinstance(l, int):\n",
        "        return itos[l]\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode('hii my name is yash'))\n",
        "print(decode(encode('hii my name is yash')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GEzfY_Ud0R63"
      },
      "outputs": [],
      "source": [
        "# import tiktoken\n",
        "\n",
        "# enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# print(enc.encode('yash'))\n",
        "# print(enc.decode(enc.encode('yash')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfV1kx_W0R63",
        "outputId": "ae864ae7-0763-4bab-cf4e-351ce8f629a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "#encode the shakespeare text\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long) #torch.long is equivalent to torch.int64\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RxPwQ6B0R63",
        "outputId": "45bb4605-a7ce-4ab4-b31b-e2f46fdb55d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854]) torch.Size([111540])\n"
          ]
        }
      ],
      "source": [
        "#train-val split 90-10\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(train_data.shape, val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "htdOAn1g0R63",
        "outputId": "90f9b85d-592d-49c3-8729-ef1abca77606"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "block_size = 8\n",
        "decode(train_data[block_size:block_size+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmnAtZV90R63",
        "outputId": "9d853f7c-53af-457f-b98d-117f31d9aeb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is F the target is i\n",
            "when input is Fi the target is r\n",
            "when input is Fir the target is s\n",
            "when input is Firs the target is t\n",
            "when input is First the target is  \n",
            "when input is First  the target is C\n",
            "when input is First C the target is i\n",
            "when input is First Ci the target is t\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size+1]\n",
        "y = train_data[1:block_size+2]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {decode(context)} the target is {decode(target)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbczFfQU0R64",
        "outputId": "7ede6472-2e49-43ea-9bf9-bfa984ceb9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "import torch\n",
        "device = torch.device(\"cpu\")  # Start with CPU as default\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    device = torch.device(\"mps\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbUylTO30R64",
        "outputId": "167377d9-2165-449c-f814-edf369f596c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "n_embed = 384\n",
        "num_heads = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "max_iters = 2000\n",
        "eval_iters = 200\n",
        "lr = 3e-4\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "K6oT0ZNA0R64"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "dkRULAdq0R64"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by non-linearity\"\"\"\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4* n_embed), # 4 multiplied based on the transformers paper scaled down version\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        #compute attention scores\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        #aggregation of values\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed) #allows the model to focus on specific aspects of the input data by projecting it into a space where relationships between different parts of the sequence can be easily assessed, making the attention mechanism more efficient and effective\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenating over the channel dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computations\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) #x + is the residual connection\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        # self.selfattention_heads = MultiHeadAttention(num_heads, n_embed//num_heads) #i.e. 4 heads of 8-dimensional self-attention\n",
        "        # self.feedforward = FeedForward(n_embed)\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     Block(n_embed, n_head = 4),\n",
        "        #     nn.LayerNorm(n_embed)\n",
        "        # )\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = num_heads) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) #final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "        # better init, covered in the follow-up original GPT video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        #idx and targets are both (B, T) tensor of integers\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #B, T, n_embed(C)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = device)) #T, n_embed\n",
        "        x = token_embeddings + position_embeddings #B, T, n_embed\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        # x = self.selfattention_heads(x) #apply one head of self-attention\n",
        "        # x = self.feedforward(x)\n",
        "        logits = self.lm_head(x) #B, T, vocab_size\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        idx = idx.to(device)\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            #get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            #focus only on last time step\n",
        "            logits = logits[:, -1, :]\n",
        "            #apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            #sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# logits, loss = model(x, y)\n",
        "\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "\n",
        "# idx = torch.zeros((1, 1), dtype=torch.long) #starting with a newline token\n",
        "# print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKiXtQK90R64",
        "outputId": "cd2127d2-f197-480a-f063-1f2683375c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/2000 [01:02<14:21:14, 25.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 3.6488, val loss 3.6718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 137/2000 [02:05<14:23,  2.16it/s]"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for steps in tqdm(range(max_iters)):\n",
        "    model.train()\n",
        "    xb, yb = get_batch('train')\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    model.to(device)\n",
        "    # print(device)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % eval_iters == 0 or steps == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsG_qL8b0R64"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long).to(device) #starting with a newline token\n",
        "print(decode(model.generate(idx, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY5RWeRx0R64"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRTo8M1l0R64"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "-np.log(1/65)\n",
        "#this is the expected loss for a random guess,\n",
        "# but since we are getting 4.27 it means our model has some extra entropy and the loss is higher than expected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItTqT9sR0R64"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs9he-nk0R64"
      },
      "outputs": [],
      "source": [
        " #toy example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single head self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) #(B,T,16)\n",
        "q = query(x) #(B, T, 16)\n",
        "v = value(x) #(B, T, 16)\n",
        "\n",
        "weights = q @ k.transpose(-2, -1) #(B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# weights = torch.zeros(T,T)\n",
        "weights = weights.masked_fill(tril==0, float('-inf'))\n",
        "weights = F.softmax(weights, dim=-1)\n",
        "out = weights @ v\n",
        "\n",
        "out.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFBR43Du0R64"
      },
      "outputs": [],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKVgwpBF0R64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}